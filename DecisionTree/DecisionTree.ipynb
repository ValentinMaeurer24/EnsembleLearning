{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Building the decision Tree from scratch**"
      ],
      "metadata": {
        "id": "C--nrj94lwRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionTree:\n",
        "\n",
        "    def __init__(self, min_samples_split=2, max_depth=5, task=\"classification\", max_features=None, random_state=None):\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.max_depth = max_depth\n",
        "        self.task = task\n",
        "        self.max_features = max_features\n",
        "        self.random_state = random_state\n",
        "\n",
        "\n",
        "    class Node:\n",
        "        def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
        "            self.feature = feature\n",
        "            self.threshold = threshold\n",
        "            self.left = left\n",
        "            self.right = right\n",
        "            self.value = value\n",
        "\n",
        "    def _split(self, dataset, feature, threshold):\n",
        "        left = dataset[dataset[:, feature] <= threshold]\n",
        "        right = dataset[dataset[:, feature] > threshold]\n",
        "        return left, right\n",
        "\n",
        "    def _gini(self, labels):\n",
        "        _, counts = np.unique(labels, return_counts=True)\n",
        "        probs = counts / labels.size\n",
        "        return 1 - np.sum(probs ** 2)\n",
        "\n",
        "    def _mse(self, values):\n",
        "        return np.mean((values - np.mean(values)) ** 2)\n",
        "\n",
        "    def _cost(self, left_labels, right_labels):\n",
        "        if self.task == \"classification\":\n",
        "            return self._gini(left_labels) + self._gini(right_labels)\n",
        "        elif self.task == \"regression\":\n",
        "            return self._mse(left_labels) + self._mse(right_labels)\n",
        "\n",
        "    def _choose_best_split(self, dataset, feature_indices):\n",
        "        best_cost, best_feature, best_threshold = np.inf, None, None\n",
        "\n",
        "        for feature in feature_indices:\n",
        "            feature_values = dataset[:, feature]\n",
        "            possible_thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in possible_thresholds:\n",
        "                left, right = self._split(dataset, feature, threshold)\n",
        "                cost = self._cost(left[:, -1], right[:, -1])\n",
        "\n",
        "                if cost < best_cost:\n",
        "                    best_cost, best_feature, best_threshold = cost, feature, threshold\n",
        "\n",
        "        return best_feature, best_threshold\n",
        "\n",
        "    def _build(self, dataset, depth):\n",
        "        n_samples, n_features = dataset.shape\n",
        "\n",
        "        if n_samples >= self.min_samples_split and depth <= self.max_depth:\n",
        "            feature_indices = np.random.choice(n_features - 1, self.max_features, replace=False) if self.max_features else np.arange(n_features - 1)\n",
        "            feature, threshold = self._choose_best_split(dataset, feature_indices)\n",
        "            if feature is not None:\n",
        "                left, right = self._split(dataset, feature, threshold)\n",
        "                left_child = self._build(left, depth + 1)\n",
        "                right_child = self._build(right, depth + 1)\n",
        "                return self.Node(feature=feature, threshold=threshold, left=left_child, right=right_child)\n",
        "\n",
        "        value = np.mean(dataset[:, -1]) if self.task == \"regression\" else np.bincount(dataset[:, -1].astype(int)).argmax()\n",
        "        return self.Node(value=value)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        dataset = np.column_stack((X, y))\n",
        "        self.root = self._build(dataset, 1)\n",
        "\n",
        "    def _predict(self, x, node):\n",
        "        if node.value is not None:\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self._predict(x, node.left)\n",
        "        else:\n",
        "            return self._predict(x, node.right)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self._predict(x, self.root) for x in X])\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {\n",
        "            \"min_samples_split\": self.min_samples_split,\n",
        "            \"max_depth\": self.max_depth,\n",
        "            \"task\": self.task,\n",
        "            \"max_features\": self.max_features,\n",
        "            \"random_state\": self.random_state,\n",
        "        }\n",
        "\n",
        "    def set_params(self, **params):\n",
        "        for key, value in params.items():\n",
        "            setattr(self, key, value)\n",
        "        return self\n"
      ],
      "metadata": {
        "id": "5Jyy9Ftil2kr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the decision tree on a classification task using the Iris dataset**"
      ],
      "metadata": {
        "id": "JD-JKuR6l7zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def cross_val_score(model, X, y, cv):\n",
        "    scores = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        model.fit(X[train_idx], y[train_idx])\n",
        "        y_pred = model.predict(X[test_idx])\n",
        "        score = accuracy_score(y[test_idx], y_pred)\n",
        "        scores.append(score)\n",
        "    return np.array(scores)\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Create a decision tree for classification \n",
        "dt = DecisionTree()\n",
        "\n",
        "# Set up cross-validation with 5 folds\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation and calculate the average accuracy\n",
        "scores = cross_val_score(dt, X, y, cv)\n",
        "print(f\"Cross-validated accuracy: {np.mean(scores):.3f} +/- {np.std(scores):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O38Bjlf1mDir",
        "outputId": "dbf803f1-cc86-4568-c02d-1b4a22a67f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validated accuracy: 0.960 +/- 0.025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of the performance of our Decision Tree on the Iris dataset**\n",
        "\n",
        "To evaluate our decision tree classifier's performance, we decided to calculate its cross-validated accuracy on a classic dataset for classification (the Iris dataset).\n",
        "\n",
        "The average accuracy score on this classification task is 96% which suggests that our decision tree performed really well and could accurately predict the species of iris flowers based on their features. \n",
        "The standard deviation of 0.025 is very low which suggests that the model is consistent in its predictions across different splits of the dataset.\n",
        "\n",
        "These results also show that the default values we chose for min_samples_split (2), max_depth (5) and max_features (None) work perfectly well for simple classification tasks\n",
        "\n",
        "In conclusion, the results indicate that the custom-built decision tree classifier is a reliable model for classifying iris flowers in the given dataset.\n"
      ],
      "metadata": {
        "id": "bNhh_DXLmXVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the decision tree on a regression task using the Auto MPG dataset**"
      ],
      "metadata": {
        "id": "Io5rejJaoc0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Auto MPG dataset\n",
        "mpg_data = sns.load_dataset(\"mpg\").dropna()\n",
        "\n",
        "# Extract features and target variable\n",
        "X = mpg_data.drop(columns=[\"origin\", \"name\", \"mpg\"])\n",
        "y = mpg_data[\"mpg\"]\n",
        "\n",
        "# Preprocessing: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "6K3G1Lnrnr5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Your updated DecisionTree class implementation goes here\n",
        "\n",
        "# Load the Auto MPG dataset\n",
        "mpg_data = sns.load_dataset(\"mpg\").dropna()\n",
        "\n",
        "# Extract features and target variable\n",
        "X = mpg_data.drop(columns=[\"origin\", \"name\", \"mpg\"])\n",
        "y = mpg_data[\"mpg\"]\n",
        "\n",
        "# Preprocessing: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    \"min_samples_split\": np.arange(2, 21),\n",
        "    \"max_depth\": np.arange(2, 21),\n",
        "    \"max_features\": [None] + list(np.arange(1, X_train.shape[1] + 1)),\n",
        "}\n",
        "\n",
        "# Create the randomized search with cross-validation\n",
        "rand_search = RandomizedSearchCV(\n",
        "    DecisionTree(task=\"regression\", random_state=42),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=100,  # Increased number of iterations\n",
        "    cv=10,  # Increased number of cross-validation folds\n",
        "    scoring=\"neg_mean_squared_error\",\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "# Fit the randomized search to the training data\n",
        "rand_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from the randomized search\n",
        "best_dt = rand_search.best_estimator_\n",
        "\n",
        "# Make predictions and evaluate the best model\n",
        "y_pred = best_dt.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", rand_search.best_params_)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R^2 Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c20hudYKzAIA",
        "outputId": "d92a4d2d-7239-4adc-e64f-c149771b7748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'min_samples_split': 17, 'max_features': 1, 'max_depth': 19}\n",
            "Mean Squared Error: 22.951057079327718\n",
            "R^2 Score: 0.5503370921239449\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.9/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluation of the performance of our Decision Tree on the Auto MPG dataset**\n",
        "\n",
        "To evaluate our decision tree regressor's performance, we calculated its MSE and R^2 score on the Auto MPG dataset, which predicts fuel efficiency based on vehicle features.\n",
        "\n",
        "The MSE of 22.95 suggests that while the model has learned patterns in the data and provides a reasonable approximation, there is still room for improvement as there is still some difference between the predicted fuel efficiency values and the actual values\n",
        "\n",
        "The R^2 score of 0.5503 suggests that our decision tree model provides a reasonable approximation of fuel efficiency. The R^2 score indicates that the model can explain approximately 55.03% of the variance in the target variable.\n",
        "\n",
        "These results demonstrate that the tuned hyperparameters min_samples_split (17), max_depth (19), and max_features (1) make the model perform relatively well on this regression task.\n",
        "\n",
        "In conclusion, our decision tree regressor is a fairly reliable model for simple regression tasks"
      ],
      "metadata": {
        "id": "ZQ8xNVdvDoUw"
      }
    }
  ]
}